{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - Dealing with imbalanced data by under or over sampling\n",
    "\n",
    "A problem with machine learning models is that they may end up biased towards the majority class, and under-predict the minority class(es).\n",
    "\n",
    "Here we look at two basic methods to correct for bias towards a majority class:\n",
    "\n",
    "1) Under-sampling the majority class\n",
    "2) Over-sampling the minority class\n",
    "\n",
    "Under-sampling the majority class may be a viable method when you know that you are working with more data than the model needs for a good fit. If you are unfamiliar with model learning curves to test for this please see:\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/11_learning_curve.ipynb\n",
    "\n",
    "Over-sampling of the minority class may also help rebalance models. Here we use simple 'bootstrapping' (repeated sampling from our population of cases), but there are more sophisticated methods, and in our next workbook we will look at one of those - Synthetic Minority Oversampling Technique (which may also increase overall accuracy as well as achieving a better balance between correct identification of minority and majority classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings (to keep notebook tidy; do not usually do this)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "\n",
    "A standard Anaconda install of Python (https://www.anaconda.com/distribution/) contains all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The section below downloads pre-processed data, and saves it to a subfolder (from where this code is run).\n",
    "If data has already been downloaded that cell may be skipped.\n",
    "\n",
    "Code that was used to pre-process the data ready for machine learning may be found at:\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is a passenger index number. We will remove this, as this is not part of the original Titanic passenger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificially reduce the number of survivors (to make data set more imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle original data\n",
    "data = data.sample(frac=1.0) # Sampling with a fraction of 1.0 shuffles data\n",
    "\n",
    "# Create masks for filters\n",
    "mask_died = data['Survived'] == 0\n",
    "mask_survived = data['Survived'] == 1\n",
    "\n",
    "# Filter data\n",
    "died = data[mask_died]\n",
    "survived = data[mask_survived]\n",
    "\n",
    "# Reduce survived by half\n",
    "survived = survived.sample(frac=0.5)\n",
    "\n",
    "# Recombine data and shuffle\n",
    "data = pd.concat([died, survived])\n",
    "data = data.sample(frac=1.0) \n",
    "\n",
    "# Show average of survived\n",
    "survival_rate = data['Survived'].mean()\n",
    "print ('Proportion survived:', np.round(survival_rate,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to measure accuracy\n",
    "\n",
    "The following is a function for multiple accuracy measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_positives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)\n",
    "\n",
    "We will separate out our features (the data we use to make a prediction) from our label (what we are truing to predict).\n",
    "By convention our features are called `X` (usually upper case to denote multiple features), and the label (survive or not) `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-sampling of majority class\n",
    "\n",
    "Here we will progressively reduce the size of the majority class examples ('died') in the training set, and look at the effect on various measures of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to reduce size of majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_majority_class(X, y, maj_sample_size=100):\n",
    "    \"\"\"\n",
    "    Identify majority class (for binary classification of y) and reduce \n",
    "    size of majority class to `n` (defaults to 100). Uses bootstrap sampling,\n",
    "    so will not error if requested sample size is greater than available \n",
    "    samples.\n",
    "    \n",
    "    Input: X and y NumPy arrays\n",
    "    Output X and y NumPy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get majority class\n",
    "    unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "    index_max = counts_elements.argmax()\n",
    "    majority_class = unique_elements[index_max]\n",
    "\n",
    "    # Separate out classes by index\n",
    "    majority_index = np.where(y == majority_class)[0]\n",
    "    minority_index = np.where(y != majority_class)[0]\n",
    "    \n",
    "    # Sample from majority class\n",
    "    restricted_sample_index = np.random.choice(\n",
    "            majority_index, maj_sample_size, replace=True)\n",
    "    \n",
    "    # Get restricted X and y for restricted data\n",
    "    X_majority_restricted = X[restricted_sample_index, :]\n",
    "    y_majority_restricted = y[restricted_sample_index]\n",
    "    \n",
    "    # Get all of minority class\n",
    "    X_minority = X[minority_index, :]\n",
    "    y_minority = y[minority_index]\n",
    "    \n",
    "    # Combine X and y \n",
    "    X_new = np.concatenate([X_majority_restricted, X_minority])\n",
    "    y_new = np.concatenate([y_majority_restricted, y_minority])\n",
    "    \n",
    "    # Shuffle (use random index to shuffle X and y in same order)\n",
    "    count_rows = X_new.shape[0]\n",
    "    random_index = np.random.permutation(count_rows)\n",
    "    X_new = X_new[random_index, :]\n",
    "    y_new = y_new[random_index]\n",
    "    \n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model with varying training size of majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NumPy arrays of X and y (required for k-fold)\n",
    "X_np = X.values\n",
    "y_np = y.values\n",
    "\n",
    "# Create lists for overall results\n",
    "results_raw_majority_count = []\n",
    "results_accuracy = []\n",
    "results_precision = []\n",
    "results_recall = []\n",
    "results_f1 = []\n",
    "results_predicted_positive_rate = []\n",
    "\n",
    "samples_sizes = list(np.arange(50, 801, 50))\n",
    "for maj_sample_size in samples_sizes:\n",
    "\n",
    "    # Set up loop for replicates for each training majority class size\n",
    "    replicate_raw_majority_count = []\n",
    "    replicate_accuracy = []\n",
    "    replicate_precision = []\n",
    "    replicate_recall = []\n",
    "    replicate_f1 = []\n",
    "    replicate_predicted_positive_rate = []\n",
    "    \n",
    "    # Set up k-fold training/test splits\n",
    "    number_of_splits = 10\n",
    "    skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "    skf.get_n_splits(X_np, y_np)\n",
    "    \n",
    "    # Loop through the k-fold splits\n",
    "    for train_index, test_index in skf.split(X_np, y_np):\n",
    "        \n",
    "        # Get X and Y train/test\n",
    "        X_train_full, X_test = X_np[train_index], X_np[test_index]\n",
    "        y_train_full, y_test = y_np[train_index], y_np[test_index]\n",
    "        \n",
    "        # Record original training majority class\n",
    "        count_of_training_died = (y_train_full == 0).sum()\n",
    "        replicate_raw_majority_count.append(count_of_training_died)\n",
    "        \n",
    "        # Get restricted training set\n",
    "        X_train, y_train = reduce_majority_class(\n",
    "                X_train_full, y_train_full, maj_sample_size)\n",
    "        \n",
    "        # Get X and Y train/test\n",
    "        X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "        \n",
    "        # Set up and fit model\n",
    "        model = LogisticRegression(solver='lbfgs')\n",
    "        model.fit(X_train_std,y_train)\n",
    "        \n",
    "        # Predict test set labels and get accuracy scores\n",
    "        y_pred_test = model.predict(X_test_std)\n",
    "        accuracy_scores = calculate_accuracy(y_test, y_pred_test)\n",
    "        replicate_accuracy.append(accuracy_scores['accuracy'])\n",
    "        replicate_precision.append(accuracy_scores['precision'])\n",
    "        replicate_recall.append(accuracy_scores['recall'])\n",
    "        replicate_f1.append(accuracy_scores['f1'])\n",
    "        replicate_predicted_positive_rate.append(\n",
    "            accuracy_scores['predicted_positive_rate'])\n",
    "                        \n",
    "    # Add mean results to overall results\n",
    "    results_raw_majority_count.append(np.mean(replicate_raw_majority_count))\n",
    "    results_accuracy.append(np.mean(replicate_accuracy))\n",
    "    results_precision.append(np.mean(replicate_precision))\n",
    "    results_recall.append(np.mean(replicate_recall))\n",
    "    results_f1.append(np.mean(replicate_f1))\n",
    "    results_predicted_positive_rate.append(\n",
    "        np.mean(replicate_predicted_positive_rate))\n",
    "\n",
    "# Transfer results to dataframe\n",
    "results = pd.DataFrame(samples_sizes, columns=['Sample_size'])\n",
    "results['accuracy'] = results_accuracy\n",
    "results['precision'] = results_precision\n",
    "results['recall'] = results_recall\n",
    "results['f1'] = results_f1\n",
    "results['predicted_positive_rate'] = results_predicted_positive_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "chart_x = results['Sample_size']\n",
    "\n",
    "plt.plot(chart_x, results['accuracy'],\n",
    "         linestyle = '-',\n",
    "         label = 'Accuracy')\n",
    "\n",
    "plt.plot(chart_x, results['precision'],\n",
    "         linestyle = '--',\n",
    "         label = 'Precision')\n",
    "\n",
    "plt.plot(chart_x, results['recall'],\n",
    "         linestyle = '-.',\n",
    "         label = 'Recall')\n",
    "\n",
    "plt.plot(chart_x, results['f1'],\n",
    "         linestyle = ':',\n",
    "         label = 'F1')\n",
    "\n",
    "plt.plot(chart_x, results['predicted_positive_rate'],\n",
    "         linestyle = '-',\n",
    "         label = 'Predicted positive rate')\n",
    "\n",
    "actual_positive_rate = np.repeat(y.mean(), len(chart_x))\n",
    "\n",
    "plt.plot(chart_x, actual_positive_rate,\n",
    "         linestyle = '--',\n",
    "         color='k',\n",
    "         label = 'Actual positive rate')\n",
    "\n",
    "# Add text showing original majority class training size\n",
    "original_majority_training_sample = int(np.mean(results_raw_majority_count))\n",
    "text = 'Original majority class training size = ' + str(\n",
    "    original_majority_training_sample)\n",
    "\n",
    "\n",
    "plt.text(80, 0.95,text, bbox=dict(facecolor='white', alpha=1.0))\n",
    "\n",
    "plt.xlabel('Majority class sample size')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(-0.02, 1.02)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Without under-sampling of the majority class, the minority class ('survived') is under-predicted.\n",
    "* By under-sampling the majority class in the training data set the model becomes more balanced, and correctly predicts the proportion of passengers who survive.\n",
    "* Balancing the model by under-sampling in this example caused a small reduction in accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling of minority class\n",
    "\n",
    "Here we will progressively increase the size of the minority class examples ('survived') in the training set, and look at the effect on various measures of accuracy. Over-sampling is achieved by repeatedly re-sampling from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to over-sample minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_majority(X, y, maj_sample_size=100):\n",
    "    \"\"\"\n",
    "    Identify majority class (for binary classification of y) and reduce \n",
    "    size of majority class to `n` (defaults to 100). Uses bootstrap sampling,\n",
    "    so will not error if requested sample size is greater than available \n",
    "    samples.\n",
    "    \n",
    "    Input: X and y NumPy arrays\n",
    "    Output X and y NumPy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get minority class\n",
    "    unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "    index_max = counts_elements.argmin()\n",
    "    minority_class = unique_elements[index_max]\n",
    "\n",
    "    # Separate out classes by index\n",
    "    minority_index = np.where(y == minority_class)[0]\n",
    "    majority_index = np.where(y != minority_class)[0]\n",
    "    \n",
    "    # Sample from majority class\n",
    "    enhanced_sample_index = np.random.choice(\n",
    "            minority_index, maj_sample_size, replace=True)\n",
    "    \n",
    "    # Get restricted X and y for restricted data\n",
    "    X_minority_enhanced = X[enhanced_sample_index, :]\n",
    "    y_minority_enhanced = y[enhanced_sample_index]\n",
    "    \n",
    "    # Get all of majority class\n",
    "    X_majority = X[majority_index, :]\n",
    "    y_majority = y[majority_index]\n",
    "    \n",
    "    # Combine X and y \n",
    "    X_new = np.concatenate([X_minority_enhanced, X_majority])\n",
    "    y_new = np.concatenate([y_minority_enhanced, y_majority])\n",
    "    \n",
    "    # Shuffle (use random index to shuffle X and y in same order)\n",
    "    count_rows = X_new.shape[0]\n",
    "    random_index = np.random.permutation(count_rows)\n",
    "    X_new = X_new[random_index, :]\n",
    "    y_new = y_new[random_index]\n",
    "    \n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model with varying training size of minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NumPy arrays of X and y (required for k-fold)\n",
    "X_np = X.values\n",
    "y_np = y.values\n",
    "\n",
    "# Create lists for overall results\n",
    "results_raw_minority_count = []\n",
    "results_accuracy = []\n",
    "results_precision = []\n",
    "results_recall = []\n",
    "results_f1 = []\n",
    "results_predicted_positive_rate = []\n",
    "\n",
    "samples_sizes = list(np.arange(50, 801, 50))\n",
    "for maj_sample_size in samples_sizes:\n",
    "\n",
    "    # Set up loop for replicates for each training majority class size\n",
    "    replicate_raw_minority_count = []\n",
    "    replicate_accuracy = []\n",
    "    replicate_precision = []\n",
    "    replicate_recall = []\n",
    "    replicate_f1 = []\n",
    "    replicate_predicted_positive_rate = []\n",
    "    \n",
    "    # Set up k-fold training/test splits\n",
    "    number_of_splits = 10\n",
    "    skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "    skf.get_n_splits(X_np, y_np)\n",
    "    \n",
    "    # Loop through the k-fold splits\n",
    "    for train_index, test_index in skf.split(X_np, y_np):\n",
    "        \n",
    "        # Get X and Y train/test\n",
    "        X_train_full, X_test = X_np[train_index], X_np[test_index]\n",
    "        y_train_full, y_test = y_np[train_index], y_np[test_index]\n",
    "        \n",
    "        # Record original training majority class\n",
    "        count_of_training_survived = (y_train_full == 1).sum()\n",
    "        replicate_raw_minority_count.append(count_of_training_survived)\n",
    "        \n",
    "        # Get enhances training set\n",
    "        X_train, y_train = increase_majority(\n",
    "                X_train_full, y_train_full, maj_sample_size)\n",
    "        \n",
    "        # Get X and Y train/test\n",
    "        X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "        \n",
    "        # Set up and fit model\n",
    "        model = LogisticRegression(solver='lbfgs')\n",
    "        model.fit(X_train_std,y_train)\n",
    "        \n",
    "        # Predict test set labels and get accuracy scores\n",
    "        y_pred_test = model.predict(X_test_std)\n",
    "        accuracy_scores = calculate_accuracy(y_test, y_pred_test)\n",
    "        replicate_accuracy.append(accuracy_scores['accuracy'])\n",
    "        replicate_precision.append(accuracy_scores['precision'])\n",
    "        replicate_recall.append(accuracy_scores['recall'])\n",
    "        replicate_f1.append(accuracy_scores['f1'])\n",
    "        replicate_predicted_positive_rate.append(\n",
    "            accuracy_scores['predicted_positive_rate'])\n",
    "                        \n",
    "    # Add mean results to overall results\n",
    "    results_raw_minority_count.append(np.mean(replicate_raw_minority_count))\n",
    "    results_accuracy.append(np.mean(replicate_accuracy))\n",
    "    results_precision.append(np.mean(replicate_precision))\n",
    "    results_recall.append(np.mean(replicate_recall))\n",
    "    results_f1.append(np.mean(replicate_f1))\n",
    "    results_predicted_positive_rate.append(\n",
    "        np.mean(replicate_predicted_positive_rate))\n",
    "\n",
    "# Transfer results to dataframe\n",
    "results = pd.DataFrame(samples_sizes, columns=['Sample_size'])\n",
    "results['accuracy'] = results_accuracy\n",
    "results['precision'] = results_precision\n",
    "results['recall'] = results_recall\n",
    "results['f1'] = results_f1\n",
    "results['predicted_positive_rate'] = results_predicted_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "chart_x = results['Sample_size']\n",
    "\n",
    "plt.plot(chart_x, results['accuracy'],\n",
    "         linestyle = '-',\n",
    "         label = 'Accuracy')\n",
    "\n",
    "plt.plot(chart_x, results['precision'],\n",
    "         linestyle = '--',\n",
    "         label = 'Precision')\n",
    "\n",
    "plt.plot(chart_x, results['recall'],\n",
    "         linestyle = '-.',\n",
    "         label = 'Recall')\n",
    "\n",
    "plt.plot(chart_x, results['f1'],\n",
    "         linestyle = ':',\n",
    "         label = 'F1')\n",
    "\n",
    "plt.plot(chart_x, results['predicted_positive_rate'],\n",
    "         linestyle = '-',\n",
    "         label = 'Predicted positive rate')\n",
    "\n",
    "actual_positive_rate = np.repeat(y.mean(), len(chart_x))\n",
    "\n",
    "plt.plot(chart_x, actual_positive_rate,\n",
    "         linestyle = '--',\n",
    "         color='k',\n",
    "         label = 'Actual positive rate')\n",
    "\n",
    "# Add text showing original majority class training size\n",
    "original_minority_training_sample = int(np.mean(results_raw_minority_count))\n",
    "text = 'Original minority class training size = ' + str(\n",
    "    original_minority_training_sample)\n",
    "\n",
    "\n",
    "plt.text(80, 0.95,text, bbox=dict(facecolor='white', alpha=1.0))\n",
    "\n",
    "plt.xlabel('Majority class sample size')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(-0.02, 1.02)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Without over-sampling of the minority class, the minority class ('survived') is under-predicted.\n",
    "* By over-sampling the majority class in the training data set the model becomes more balanced, and correctly predicts the proportion of passengers who survive.\n",
    "* Balancing the model by over-sampling in this example had little effect of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall observations\n",
    "\n",
    "Both under-sampling of the majority class, or over-sampling of the minority class, can balance the performance of the model across different classes. Balancing the model leads to better prediction of the proportion in each class (e.g. 'survived' or 'died'), but this may come at a small loss in overall accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
