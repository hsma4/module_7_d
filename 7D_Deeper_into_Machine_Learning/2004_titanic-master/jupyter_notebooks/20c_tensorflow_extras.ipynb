{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - TensorFlow extra functionality\n",
    "\n",
    "In this example we show extra TensorFlow/Keras functionality, inlcuding\n",
    "\n",
    "* Changing class weights, e.g. to increase weight of minroty class(es)\n",
    "* Saving model checkpoints while training (and only saving models with improved validation accuracy)\n",
    "* Early stop of model (when no improvement)\n",
    "* Getting model weights\n",
    "* Saving and reloading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TensorFlow api model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = False\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scale data\n",
    "\n",
    "In neural networks it is common to to scale input data 0-1 rather than use standardisation (subtracting mean and dividing by standard deviation) of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net\n",
    "\n",
    "Here we use the api-based method to set up a TensorFlow neural network. This method allows us\n",
    "to more flexibly define the inputs for each layer, rather than assuming there is a simple sequence as with the Sequential method.\n",
    "\n",
    "We will put construction of the neural net into a separate function.\n",
    "\n",
    "The neural net is a relatively simple network. The inputs are connected to two hidden layers (of 240 and 50 nodes) before being connected to two output nodes corresponding to each class (died and survived). It also contains some useful additions (batch normalisation and dropout) as described below.\n",
    "The layers of the network are:\n",
    "\n",
    "1) An input layer (which does need to be defined)\n",
    "\n",
    "2) A fully-connected (dense) layer.This is defined by the number of inputs (the number of input features) and the number of outputs. We will expand out feature data set up to 240 outputs. The output of the layer uses ReLU (rectified linear unit) activation. ReLU activation is most common for the inner layers of a neural network. Negative input values are set to zero. Positive input values are left unchanged.\n",
    "\n",
    "3) A batch normalisation layer. This is not usually used for small models, but can increase the speed of training for larger models. It is added here as an example of how to include it (in large models all dense layers would be followed by a batch normalisation layer). The layer definition includes the number of inputs to normalise. \n",
    "\n",
    "4) A dropout layer. This layer randomly sets outputs from the preceding layer to zero during training (a different set of outputs is zeroed for each training iteration). This helps prevent over-fitting of the model to the training data. Typically between 0.1 and 0.3 outputs are set to zero (p=0.1 means 10% of outputs are set to zero).\n",
    "\n",
    "5) A second fully connected layer which reduces the network down to 50 nodes. This again uses ReLU activation and is followed by batch normalisation, and dropout layers.\n",
    "\n",
    "6) A final fully connected linear layer of one nodes (more nodes could be used for more classes, in which case use softmax activation and categorical_crossentropy in the loss function). The output of the net is the probability of surviving (usually a probability of >= 0.5 will be classes as ‘survived’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, learning_rate=0.003):\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Define layers\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    dense_1 = layers.Dense(240, activation='relu')(inputs)\n",
    "    norm_1 = layers.BatchNormalization()(dense_1)\n",
    "    dropout_1 = layers.Dropout(0.25)(norm_1)\n",
    "    dense_2 = layers.Dense(50, activation='relu')(dropout_1)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(dense_2)\n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    net.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary of the model structure\n",
    "\n",
    "Here we will create a model with 10 input features and show the structure of the model as  atable and as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary pip or conda install pydot and graphviz\n",
    "keras.utils.plot_model(model, \"titanic_tf_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dictionary of weights according to class distribution\n",
    "\n",
    "Here we will create a dictionary of class weights, in inverse proportion to how frequently a class occurs (to less frequent classes will be given more weight in model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion of each class\n",
    "prop_class_1 = y.mean()\n",
    "prop_class_0 = 1 - prop_class_1\n",
    "\n",
    "# Set weights in inverse propotion to occurance\n",
    "weight_class_0 = 1 / prop_class_0\n",
    "weight_class_1 = 1 / prop_class_1\n",
    "\n",
    "# Create dictionary of class weight\n",
    "class_weights = {0: weight_class_0, 1: weight_class_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "TensorFlow can track the history of training, enabling us to examine performance against training and test sets over time. Here we will use the same model as above, but without k-fold validation and with history tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_np, y_np, test_size = 0.25)\n",
    "\n",
    "# Scale data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "\n",
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('model_checkpoint.h5',\n",
    "                                                save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs)\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=25,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# Train model (including class weights)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    class_weight = class_weights,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Custom callbacks may also be defined. \n",
    "# These may run on training start/end, on epoch start/end, or bacth start/end.\n",
    "# For more on callbacks, see: https://keras.io/callbacks/\n",
    "\n",
    "# Reload model with model = keras.models.load_model('model_name.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`history` is a dictionary containing data collected during training. Let's take a look at the keys in this dictionary (these are the metrics monitored during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model weights\n",
    "\n",
    "Here we show how weights for a layer can be extracted from the model if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights() # Biases are not used in this model\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reloading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('titanic_tf_model.h5')\n",
    "\n",
    "# Load and use saved model\n",
    "restored_model = keras.models.load_model('titanic_tf_model.h5')\n",
    " \n",
    "# Predict classes\n",
    "predicted_proba = restored_model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom callbacks\n",
    "\n",
    "Callbacks can be called at various points:\n",
    "\n",
    "* `on_train_begin()`\n",
    "* `on_train_end()`\n",
    "* `on_epoch_begin()`\n",
    "* `on_epoch_end()`\n",
    "* `on_batch_begin()`\n",
    "* `on_batch_end()`\n",
    "\n",
    "For evaluation (called by `evaluate()` there is:\n",
    "\n",
    "* `on_test_begin()`\n",
    "* `on_test_end()`\n",
    "* `on_test_batch_begin()`\n",
    "* `on_test_batch_end()`\n",
    "\n",
    "For prediction (called by `predict()` there is:\n",
    "\n",
    "* `on_predict_begin()`\n",
    "* `on_predict_end()`\n",
    "* `on_predict_batch_begin()`\n",
    "* `on_predict_batch_end()`\n",
    "\n",
    "Here we will implement a callback that prints the ratio of validation to training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        ratio = logs['val_loss'] / logs['loss']\n",
    "        print (f' Val/Train: {ratio:.2f}')\n",
    "        \n",
    "checkpoint_ratio = PrintValTrainRatioCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we fit the model (using verbose fit to print progress). We see the val/train ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (including class weights)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    class_weight = class_weights,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                        checkpoint_cb, early_stopping_cb, checkpoint_ratio])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
