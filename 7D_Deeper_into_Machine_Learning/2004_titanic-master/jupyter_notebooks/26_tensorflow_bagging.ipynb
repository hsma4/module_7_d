{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - Bagging\n",
    "\n",
    "'Bagging' (bootstrap aggregation') is a simple but powerful method for improving accuracy of a model, and for getting a measure of model uncertainty.\n",
    "\n",
    "The principle is simple: we train *n* neural nets all using different bootstrapped samples (sampling with replacement) from the training set. We then use the mean probability of classification from the ensemble of models.\n",
    "\n",
    "Breiman, L. Bagging Predictors. Machine Learning 24, 123â€“140 (1996). https://doi.org/10.1023/A:1018054314350\n",
    "\n",
    "This method may provide better performance than Monte-Carlo Dropout (see separate notebook), but is signficantly more computationally expensive as training needs to be repeated for each net in the ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TensorFlow api model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scale data\n",
    "\n",
    "In neural networks it is common to to scale input data 0-1 rather than use standardisation (subtracting mean and dividing by standard deviation) of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, learning_rate=0.003):\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Define layers\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    dropout_0 = layers.Dropout(0.2)(inputs)\n",
    "    \n",
    "    dense_1 = layers.Dense(240, activation='relu')(dropout_0)\n",
    "    dropout_1 = layers.Dropout(0.2)(dense_1)\n",
    "    \n",
    "    dense_2 = layers.Dense(50, activation='relu')(dropout_1)\n",
    "    dropout_2 = layers.Dropout(0.2)(dense_2)\n",
    "  \n",
    "    outputs = layers.Dense(1, activation='sigmoid')(dropout_2)\n",
    "    \n",
    "    \n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    net.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary of the model structure\n",
    "\n",
    "Here we will create a model with 10 input features and show the structure of the model as  atable and as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the plot of the model shows how the imput layer is connected to both the first dense layer and the concatenation layer prior to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary conda install pydot and graphviz\n",
    "keras.utils.plot_model(model, \"titanic_tf_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_np, y_np, test_size = 0.25)\n",
    "\n",
    "# Scale data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "single_model = make_net(number_features)\n",
    "\n",
    "# Train model\n",
    "single_model.fit(X_train_sc,\n",
    "            y_train,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an ensemble of models\n",
    "\n",
    "Training is as a single model, but we use bootstrap sampling (sampling with replacement) to create different training sets.\n",
    "\n",
    "Here we will use a fixed number (25) of nets. In practice this number should be a adjusted until a plateau in performance is reached. It is also possible to vary the bottstrap saple size - here we fix the sample size to be the same size as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nets = 25\n",
    "training_set_size = len(X_train_sc)\n",
    "sample_size = training_set_size\n",
    "\n",
    "# Create and train models\n",
    "models = []\n",
    "number_features = X_train_sc.shape[1]\n",
    "for i in range (number_of_nets):\n",
    "    # Set up models\n",
    "    print (f'Training model {i+1} of {number_of_nets}')\n",
    "    model = make_net(number_features)\n",
    "    # Get samples of training data\n",
    "    indexes = np.random.choice(range(training_set_size), sample_size)\n",
    "    resampled_X_train_sc = X_train_sc[indexes]\n",
    "    resampled_y_train = y_train[indexes]\n",
    "    # Fit model\n",
    "    model.fit(resampled_X_train_sc,\n",
    "            resampled_y_train,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=0)\n",
    "    # Add model to list of models\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas_bagging = np.stack(\n",
    "        [models[i](X_test_sc) for i in range (number_of_nets)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean probabilities, and classify those with mean >= 0.5 as surviving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_bagging = y_probas_bagging.mean(axis=0)\n",
    "y_predict_bagging = y_proba_bagging >= 0.5\n",
    "y_predict_bagging = y_predict_bagging.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get normal predictions from single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normal predictions (single model) \n",
    "y_proba = single_model.predict(X_test_sc)\n",
    "y_predict = y_proba >= 0.5\n",
    "y_predict = y_predict.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show accuracy scores (we are doing a single test set here; in practice k-fold validation should be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(y_predict == y_test)\n",
    "accuracy_bagging = np.mean(y_predict_bagging == y_test)\n",
    "print (f'Accuracy of single net {accuracy:0.2f}')\n",
    "print (f'Accuracy of bagged nets {accuracy_bagging:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot comparisons of dropout method and standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_error = y_probas_bagging.std(axis=0)/np.sqrt(number_of_nets)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.errorbar(y_proba, y_proba_bagging, yerr = y_error, fmt='o')\n",
    "ax.grid()\n",
    "ax.set_xlabel('Probability survival (single network)')\n",
    "ax.set_ylabel('Probability survival (bagging)')\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show variation in prediction (across the dropout replicates) depending on probability of survival.\n",
    "\n",
    "Here we will use standard error of the mean as one measure of uncertainty. Note how uncertainty is not even - there is less variation in prediction of those with high probability of survival compared with those with lower probability of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(y_proba, y_error)\n",
    "ax.set_xlabel('Probability survival')\n",
    "ax.set_ylabel('Standard Error of Survival Probability')\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
