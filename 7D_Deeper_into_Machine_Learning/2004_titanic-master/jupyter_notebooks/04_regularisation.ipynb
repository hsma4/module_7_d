{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - Regularisation\n",
    "\n",
    "A danger with complex models (many features) or small data sets (a low number of samples) is that the model can over-fit to the training data at the expense of previously unseen data (as in the test set). Most machine learning approaches allow for use of some kind of 'regularisation' which reduces the strength of the fit to the training data (e.g. by reducing the values of the model weights/coefficients). While this reduces the accuracy of the fit to the training data it can, perhaps surprisingly, increase the accuracy of predicting test (or other previously unseen) data.\n",
    "\n",
    "Over-fitting is usually spotted by the accuracy of prediction being significantly higher for the training set compared to the test set.\n",
    "\n",
    "Here we will deliberately reduce the number of samples in the Titanic data set, and increase the number of features with polynomial expansion*, to exaggerate the problem of over-fitting, and show how regularisation can help.\n",
    "\n",
    "Note: This workbook follows on from previous workbooks on logistic regression and stratified k-fold validation, both of which are used here,\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/02_logistic_regression.ipynb\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/03_k_fold.ipynb\n",
    "\n",
    "*When we use polynomial expansion of features, we create new features that are the product of two features. For example if we had two features, A and B, we would produce the following extra features:\n",
    "* A*A\n",
    "* A*B\n",
    "* B*A\n",
    "* B*B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Run the following code if data for Titanic survival has not been previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into two DataFrames\n",
    "X_df = data.drop('Survived',axis=1)\n",
    "y_df = data['Survived']\n",
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the number of samples, and increase the number of features\n",
    "\n",
    "Now we will reduce the size of the data set using random sampling (using Pandas `sample` method). \n",
    "We will increase the number of features using polynomial expansion (creating products of each pair of features).\n",
    "\n",
    "This is to help show the effect of over-fitting, as small data sets are more susceptible to over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of samples\n",
    "data = data.sample(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "X = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to standardise data\n",
    "Standardisation subtracts the mean and divides by the standard deviation, for each feature.\n",
    "Here we use the sklearn built-in method for standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the model for all k-fold splits\n",
    "\n",
    "The following code:\n",
    "* Defines a list of regularisation (lower values lead to greater regularisation)\n",
    "* Sets up lists to hold results for each k-fold split\n",
    "* Starts a loop for each regularisation value, and loops through:\n",
    "    * Print regularisation level (to show progress)\n",
    "    * Sets up lists to record replicates from k-fold stratification\n",
    "    * Sets up the k-fold splits using sklearn's `StratifiedKFold` method\n",
    "    * Trains a logistic regression model, and test its it, for each k-fold split\n",
    "    * Adds each k-fold training/test accuracy to the lists\n",
    "* Record average accuracy from k-fold stratification (so each regularisation level has one accuracy result recorded for training and test sets)\n",
    "\n",
    "We pass the regularisation to the model during fitting, it has the argument name `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_values = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "# Set up lists to hold results\n",
    "training_acc_results = []\n",
    "test_acc_results = []\n",
    "\n",
    "# Set up splits\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# Set up model type\n",
    "\n",
    "for reg in reg_values:\n",
    "    # Show progress\n",
    "    print(reg, end=' ')\n",
    "    \n",
    "    # Set up lists for results for each of k splits\n",
    "    training_k_results = []\n",
    "    test_k_results = []\n",
    "    # Loop through the k-fold splits\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Get X and Y train/test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Standardise X data\n",
    "        X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "        # Fit model with regularisation (C)\n",
    "        model = LogisticRegression(C=reg, solver='lbfgs', max_iter=1000)\n",
    "        model.fit(X_train_std,y_train)\n",
    "        # Predict training and test set labels\n",
    "        y_pred_train = model.predict(X_train_std)\n",
    "        y_pred_test = model.predict(X_test_std)\n",
    "        # Calculate accuracy of training and test sets\n",
    "        accuracy_train = np.mean(y_pred_train == y_train)\n",
    "        accuracy_test = np.mean(y_pred_test == y_test)\n",
    "        # Record accuracy for each k-fold split\n",
    "        training_k_results.append(accuracy_train)\n",
    "        test_k_results.append(accuracy_test)\n",
    "    # Record average accuracy for each k-fold split\n",
    "    training_acc_results.append(np.mean(training_k_results))\n",
    "    test_acc_results.append(np.mean(test_k_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define data for chart\n",
    "x = reg_values\n",
    "y1 = training_acc_results\n",
    "y2 = test_acc_results\n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Plot training set accuracy\n",
    "ax1.plot(x, y1,\n",
    "        color = 'k',\n",
    "        linestyle = '-',\n",
    "        markersize = 8,\n",
    "        marker = 'o',\n",
    "        markerfacecolor='k',\n",
    "        markeredgecolor='k',\n",
    "        label  = 'Training set accuracy')\n",
    "\n",
    "# Plot test set accuracy\n",
    "ax1.plot(x, y2,\n",
    "        color = 'r',\n",
    "        linestyle = '-',\n",
    "        markersize = 8,\n",
    "        marker = 'o',\n",
    "        markerfacecolor='r',\n",
    "        markeredgecolor='r',\n",
    "        label  = 'Test set accuracy')\n",
    "\n",
    "# Custimise axes\n",
    "ax1.grid(True, which='both')\n",
    "ax1.set_xlabel('Regularisation\\n(lower value = greater regularisation)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Add legend\n",
    "ax1.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above figure that:\n",
    "\n",
    "1) Accuracy of training set is significantly higher than accuracy of test set (a common sign of over-fitting).\n",
    "\n",
    "2) There is an optimal value for the regularisation constant, C. In this case that value is about 0.1 (default if not specified is 1.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
