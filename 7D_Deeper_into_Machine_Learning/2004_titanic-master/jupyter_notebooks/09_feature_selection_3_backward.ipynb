{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - feature selection 3 (model backward elimination)\n",
    "\n",
    "Reducing the number of features we use can have three benefits:\n",
    "\n",
    "* Simplifies model explanation\n",
    "* Model fit may be improved by the removal of features that add no value\n",
    "* Model will be faster to fit\n",
    "\n",
    "In this notebook we will use a model-based approach whereby we incrementally remove features that least reduce model performance (we could use simple accuracy, but in this case we will use ROC Area Under Curve as a more thorough analysis of performance). If you are not familiar with ROC AUC, have a look at:\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/06_roc_sensitivity_specificity.ipynb\n",
    "\n",
    "Two key advantages of this method are:\n",
    "\n",
    "* It is relatively simple.\n",
    "* It is tailored to the model in question.\n",
    "\n",
    "Some key disadvantage of this method are:\n",
    "\n",
    "* It may be slow if there are many parameters (though the loop to select features could be limited in the number of features to select).\n",
    "* The selection of features may be dependent on model meta-parameters (such as level of regularisation).\n",
    "* The selection of features may not transfer between models (e.g. a model that does not allow for feature interactions may not detect features which do not add much value independently).\n",
    "\n",
    "The machine learning model we will use to test the feature selection is as previously described:\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/02_logistic_regression.ipynb\n",
    "\n",
    "We will also assess performance using k-fold stratification for better measurement of performance. If you are not familiar with k-fold stratification, have a look at:\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/03_k_fold.ipynb\n",
    "\n",
    "This method is the mirror image of the method that incrementally adds features. The best features *should* be very similar in both methods, but if both methods are run then it may be best to combine lists of best features if they differ a little. Here is the method for choosing features by incrementally adding those features that lead to best improvement in model performance:\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/08_feature_selection_2_forward.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the following steps:\n",
    "\n",
    "* Download and save pre-processed data\n",
    "* Split data into features (X) and label (y)\n",
    "* Loop through features to select the feature that most increas ROC AUC\n",
    "* Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings (to keep notebook tidy; do not usually do this)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "\n",
    "A standard Anaconda install of Python (https://www.anaconda.com/distribution/) contains all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The section below downloads pre-processed data, and saves it to a subfolder (from where this code is run).\n",
    "If data has already been downloaded that cell may be skipped.\n",
    "\n",
    "Code that was used to pre-process the data ready for machine learning may be found at:\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is a passenger index number. We will remove this, as this is not part of the original Titanic passenger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)\n",
    "\n",
    "We will separate out our features (the data we use to make a prediction) from our label (what we are truing to predict).\n",
    "By convention our features are called `X` (usually upper case to denote multiple features), and the label (survive or not) `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data standardisation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward selection method:\n",
    "\n",
    "* Keeps a list of selected features\n",
    "* Keeps a list of features still available for selection\n",
    "* Loops through available features:\n",
    "    * Calculates added value for each feature (using stratified k-fold validation)\n",
    "    * Selects feature that adds most value\n",
    "    * Adds selected feature to *selected features* list and removes it from *available features* list\n",
    "    \n",
    "This method uses a `while` lop to keep exploring features until no more are available. An alternative would be to use a `for` loop with a maximum number of features to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store accuracies and chosen features\n",
    "roc_auc_by_feature_number = []\n",
    "chosen_features = []\n",
    "\n",
    "# Initialise chosen features list and run tracker\n",
    "available_features = list(X)\n",
    "run = 0\n",
    "number_of_features = len(list(X))\n",
    "\n",
    "# Creat einitial reference performance\n",
    "reference_auc = 1.0 # used to compare reduction in AUC\n",
    "\n",
    "# Loop through feature list to select next feature\n",
    "while len(available_features)> 1:\n",
    "\n",
    "    # Track and pront progress\n",
    "    run += 1\n",
    "    print ('Feature run {} of {}'.format(run, number_of_features-1))\n",
    "    \n",
    "    # Convert DataFrames to NumPy arrays\n",
    "    y_np = y.values\n",
    "    \n",
    "    # Reset best feature and accuracy\n",
    "    best_result = 1.0\n",
    "    best_feature = ''\n",
    "\n",
    "    # Loop through available features\n",
    "    for feature in available_features:\n",
    "\n",
    "        # Create copy of already chosen features to avoid original being changed\n",
    "        features_to_use = available_features.copy()\n",
    "        # Create a list of features to use by removing 1 feature\n",
    "        features_to_use.remove(feature)\n",
    "        # Get data for features, and convert to NumPy array\n",
    "        X_np = X[features_to_use].values\n",
    "        \n",
    "        # Set up lists to hold results for each selected features\n",
    "        test_auc_results = []\n",
    "    \n",
    "        # Set up k-fold training/test splits\n",
    "        number_of_splits = 5\n",
    "        skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "        skf.get_n_splits(X_np, y)\n",
    "    \n",
    "        # Loop through the k-fold splits\n",
    "        for train_index, test_index in skf.split(X_np, y_np):\n",
    "            \n",
    "            # Get X and Y train/test\n",
    "            X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "            # Get X and Y train/test\n",
    "            X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "    \n",
    "            # Set up and fit model\n",
    "            model = LogisticRegression(solver='lbfgs')\n",
    "            model.fit(X_train_std,y_train)\n",
    "    \n",
    "            # Predict test set labels\n",
    "            y_pred_test = model.predict(X_test_std)\n",
    "            \n",
    "            # Calculate accuracy of test sets\n",
    "            accuracy_test = np.mean(y_pred_test == y_test)\n",
    "          \n",
    "            # Get ROC AUC\n",
    "            probabilities = model.predict_proba(X_test_std)\n",
    "            probabilities = probabilities[:, 1] # Probability of 'survived'\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            test_auc_results.append(roc_auc)\n",
    "        \n",
    "        # Get average result from all k-fold splits\n",
    "        feature_auc = np.mean(test_auc_results)\n",
    "    \n",
    "        # Update chosen feature and result if this feature is a new best\n",
    "        # We are looking for the smallest drop in performance\n",
    "        drop_in_performance = reference_auc - feature_auc\n",
    "        if drop_in_performance < best_result:\n",
    "            best_result = drop_in_performance\n",
    "            best_feature = feature\n",
    "            best_auc = feature_auc\n",
    "                \n",
    "    # k-fold splits are complete    \n",
    "    # Add mean accuracy and AUC to record of accuracy by feature number\n",
    "    roc_auc_by_feature_number.append(best_auc)\n",
    "    chosen_features.append(best_feature)    \n",
    "    available_features.remove(best_feature)\n",
    "    reference_auc = best_auc\n",
    "\n",
    "# Add last remaining feature\n",
    "chosen_features += available_features\n",
    "roc_auc_by_feature_number.append(0)\n",
    "    \n",
    "# Put results in DataFrame\n",
    "# Reverse order of lists with [::-1] so best features first\n",
    "results = pd.DataFrame()\n",
    "results['feature removed'] = chosen_features[::-1]\n",
    "results['ROC AUC'] = roc_auc_by_feature_number[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results\n",
    "The table is now in the order of preferred features, though our code worked in the reverse direction, incrementally removing the feature that made least difference to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "chart_x = list(range(1, number_of_features+1))\n",
    "\n",
    "plt.plot(chart_x, roc_auc_by_feature_number,\n",
    "        label = 'ROC AUC')\n",
    "\n",
    "plt.xlabel('Number of features removed')\n",
    "plt.ylabel('Accuracy (ROC AUC)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results it looks like we could eliminate all but 5-6 features in this model. It may also be worth examining the same method using other performance scores (such as simple accuracy, or f1) in place of ROC AUC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
