{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - Feature expansion\n",
    "\n",
    "Spoiler Alert! By combining feature expansion and feature selection we can increase our logistic regression model accuracy (on previously unseen data) from 79% to 84%!\n",
    "\n",
    "Simple models such as logistic regression do not incorporate complex interactions between features. If two features produce more than an additive effect, this will not be fitted in logistic regression. In order to allow for feature interaction we need to add terms that create new features by producing the product of each product pair.\n",
    "\n",
    "When we use polynomial expansion of features, we create new features that are the product of two features. For example if we had two features, A, B and C, a full polynomial expansion would produce the following extra features:\n",
    "\n",
    "* A.A, A.B, A.C\n",
    "* B.A, B.B, B.C\n",
    "* C.A, C.B, C.C\n",
    "\n",
    "But we will reduce this in two ways:\n",
    "\n",
    "* Remove duplicate terms (e.g. A.B and B.A are the same, so we only need A.B)\n",
    "* Use the `interaction_only` argument to remove powers of single features (e.g. A.A, B.B)\n",
    "\n",
    "A danger of polynomial expansion is that the model may start to over-fit to the training data. This may be dealt with in one (or both of two ways):\n",
    "\n",
    "* Increase the regularisation strength in the model (reduce the value of `C` in the logistic regression model)\n",
    "* Use feature selection to pick only the most important features (which now may include polynomial features)\n",
    "\n",
    "The methods described here build on previous notebooks, notably on logistic regression, k-fold, sampling, regularisation, and model-based forward feature selection\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/02_logistic_regression.ipynb\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/03_k_fold.ipynb\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/04_regularisation.ipynb\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/08_feature_selection_2_forward.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Run the following code if data for Titanic survival has not been previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into two DataFrames\n",
    "X_df = data.drop('Survived',axis=1)\n",
    "y_df = data['Survived']\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of our data sets (the first value is the number of samples, and the second value is the number of features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Shape of X:', X.shape)\n",
    "print ('Shape of X_poly:', X_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah - we've gone from 24 features to 301! But are they any use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing normal and expanded models with varying regularisation\n",
    "\n",
    "The following code:\n",
    "* Defines a list of regularisation (lower values lead to greater regularisation)\n",
    "* Sets up lists to hold results for each k-fold split\n",
    "* Starts a loop for each regularisation value, and loops through:\n",
    "    * Print regularisation level (to show progress)\n",
    "    * Sets up lists to record replicates from k-fold stratification\n",
    "    * Sets up the k-fold splits using sklearn's `StratifiedKFold` method\n",
    "    * Trains two logistic regression models (regular and polynomial), and test its it, for each k-fold split\n",
    "    * Adds each k-fold training/test accuracy to the lists\n",
    "* Record average accuracy from k-fold stratification (so each regularisation level has one accuracy result recorded for training and test sets)\n",
    "\n",
    "We pass the regularisation to the model during fitting, it has the argument name `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to standardise data\n",
    "\n",
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing normal and polynomial models\n",
    "\n",
    "reg_values = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "# Set up lists to hold results\n",
    "training_acc_results = []\n",
    "test_acc_results = []\n",
    "training_acc_results_poly = []\n",
    "test_acc_results_poly = []\n",
    "\n",
    "# Set up splits\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "skf.get_n_splits(X, y)\n",
    "skf.get_n_splits(X_poly, y)\n",
    "\n",
    "# Set up model type\n",
    "\n",
    "for reg in reg_values:\n",
    "    # Show progress\n",
    "    print(reg, end=' ')\n",
    "    \n",
    "    # Set up lists for results for each of k splits\n",
    "    training_k_results = []\n",
    "    test_k_results = []\n",
    "    training_k_results_poly = []\n",
    "    test_k_results_poly = []\n",
    "    # Loop through the k-fold splits\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        # Normal (non-polynomial model)\n",
    "        \n",
    "        # Get X and Y train/test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Standardise X data\n",
    "        X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "        # Fit model with regularisation (C)\n",
    "        model = LogisticRegression(C=reg, solver='lbfgs', max_iter=1000)\n",
    "        model.fit(X_train_std,y_train)\n",
    "        # Predict training and test set labels\n",
    "        y_pred_train = model.predict(X_train_std)\n",
    "        y_pred_test = model.predict(X_test_std)\n",
    "        # Calculate accuracy of training and test sets\n",
    "        accuracy_train = np.mean(y_pred_train == y_train)\n",
    "        accuracy_test = np.mean(y_pred_test == y_test)\n",
    "        # Record accuracy for each k-fold split\n",
    "        training_k_results.append(accuracy_train)\n",
    "        test_k_results.append(accuracy_test)\n",
    "        \n",
    "        # Polynomial model (same as above except use X with polynomial features)\n",
    "        \n",
    "        # Get X and Y train/test\n",
    "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Standardise X data\n",
    "        X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "        # Fit model with regularisation (C)\n",
    "        model = LogisticRegression(C=reg, solver='lbfgs', max_iter=1000)\n",
    "        model.fit(X_train_std,y_train)\n",
    "        # Predict training and test set labels\n",
    "        y_pred_train = model.predict(X_train_std)\n",
    "        y_pred_test = model.predict(X_test_std)\n",
    "        # Calculate accuracy of training and test sets\n",
    "        accuracy_train = np.mean(y_pred_train == y_train)\n",
    "        accuracy_test = np.mean(y_pred_test == y_test)\n",
    "        # Record accuracy for each k-fold split\n",
    "        training_k_results_poly.append(accuracy_train)\n",
    "        test_k_results_poly.append(accuracy_test)\n",
    "        \n",
    "    # Record average accuracy for each k-fold split\n",
    "    training_acc_results.append(np.mean(training_k_results))\n",
    "    test_acc_results.append(np.mean(test_k_results))\n",
    "    training_acc_results_poly.append(np.mean(training_k_results_poly))\n",
    "    test_acc_results_poly.append(np.mean(test_k_results_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define data for chart\n",
    "x = reg_values\n",
    "y1 = training_acc_results\n",
    "y2 = test_acc_results\n",
    "y3 = training_acc_results_poly\n",
    "y4 = test_acc_results_poly\n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Plot training set accuracy\n",
    "ax1.plot(x, y1,\n",
    "        color = 'k',\n",
    "        linestyle = '-',\n",
    "        markersize = 8,\n",
    "        marker = 'o',\n",
    "        markerfacecolor='k',\n",
    "        markeredgecolor='k',\n",
    "        label  = 'Training set accuracy')\n",
    "\n",
    "# Plot test set accuracy\n",
    "ax1.plot(x, y2,\n",
    "        color = 'r',\n",
    "        linestyle = '-',\n",
    "        markersize = 8,\n",
    "        marker = 'o',\n",
    "        markerfacecolor='r',\n",
    "        markeredgecolor='r',\n",
    "        label  = 'Test set accuracy')\n",
    "\n",
    "# Plot training set accuracy (poly model)\n",
    "ax1.plot(x, y3,\n",
    "        color = 'g',\n",
    "        linestyle = '-',\n",
    "        markersize = 8,\n",
    "        marker = 'o',\n",
    "        markerfacecolor='g',\n",
    "        markeredgecolor='g',\n",
    "        label  = 'Training set accuracy (poly)')\n",
    "\n",
    "# Plot test set accuracy (poly model)\n",
    "ax1.plot(x, y4,\n",
    "        color = 'b',\n",
    "        linestyle = '-',\n",
    "        markersize = 8,\n",
    "        marker = 'o',\n",
    "        markerfacecolor='b',\n",
    "        markeredgecolor='b',\n",
    "        label  = 'Test set accuracy (poly)')\n",
    "\n",
    "# Customise axes\n",
    "ax1.grid(True, which='both')\n",
    "ax1.set_xlabel('Regularisation\\n(lower value = greater regularisation)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Add legend\n",
    "ax1.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show best test set accuracy measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_non_poly = np.max(test_acc_results)\n",
    "best_test_poly = np.max(test_acc_results_poly)\n",
    "print ('Best accuracy for non-poly and poly were {0:0.3f} and {1:0.3f}'.format(\n",
    "    best_test_non_poly, best_test_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above figure that:\n",
    "\n",
    "* Polynomial expansion has increased the accuracy of both training and test sets. Test set accuracy was increased over 2%\n",
    "* We do not know which polynomial terms are most useful (below we will use feature reduction to identify those)\n",
    "* The polynomial X data suffers from more over-fitting than the non-polynomial set (there is a larger difference between training and test set accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction after feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will revisit the code we have used previously to pick the best features.\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/08_feature_selection_2_forward.ipynb\n",
    "\n",
    "In the previous method we ranked all features in their ability to improve model performance. Here, because there are many more features we will look at the influence of the top 20 (and if we see model performance is still increasing with additional features we could come back and change that limit).\n",
    "\n",
    "We will amend the previous code as well to use simple accuracy (rather than the ROC Area Under Curve in our previous example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer polynomial X into a pandas DataFrame (as method use Pandas)\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names())\n",
    "\n",
    "# Create list to store accuracies and chosen features\n",
    "accuracy_by_feature_number = []\n",
    "chosen_features = []\n",
    "\n",
    "# Initialise chosen features list and run tracker\n",
    "available_features = list(poly.get_feature_names())\n",
    "run = 0\n",
    "number_of_features = len(list(X))\n",
    "\n",
    "# Loop through feature list to select next feature\n",
    "maximum_features_to_choose = 20\n",
    "\n",
    "for i in range(maximum_features_to_choose):\n",
    "\n",
    "    # Track and pront progress\n",
    "    run += 1\n",
    "    print ('Feature run {} of {}'.format(run, maximum_features_to_choose))\n",
    "    \n",
    "    # Reset best feature and accuracy\n",
    "    best_result = 0\n",
    "    best_feature = ''\n",
    "\n",
    "    # Loop through available features\n",
    "    for feature in available_features:\n",
    "\n",
    "        # Create copy of already chosen features to avoid original being changed\n",
    "        features_to_use = chosen_features.copy()\n",
    "        # Create a list of features from features already chosen + 1 new feature\n",
    "        features_to_use.append(feature)\n",
    "        # Get data for features, and convert to NumPy array\n",
    "        X_np = X_poly_df[features_to_use].values\n",
    "        \n",
    "        # Set up lists to hold results for each selected features\n",
    "        test_accuracy_results = []\n",
    "    \n",
    "        # Set up k-fold training/test splits\n",
    "        number_of_splits = 5\n",
    "        skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "        skf.get_n_splits(X_np, y)\n",
    "    \n",
    "        # Loop through the k-fold splits\n",
    "        for train_index, test_index in skf.split(X_np, y):\n",
    "            \n",
    "            # Get X and Y train/test\n",
    "            X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "            # Get X and Y train/test\n",
    "            X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "    \n",
    "            # Set up and fit model\n",
    "            model = LogisticRegression(solver='lbfgs')\n",
    "            model.fit(X_train_std,y_train)\n",
    "    \n",
    "            # Predict test set labels\n",
    "            y_pred_test = model.predict(X_test_std)\n",
    "                        \n",
    "            # Calculate accuracy of test sets\n",
    "            accuracy_test = np.mean(y_pred_test == y_test)\n",
    "            test_accuracy_results.append(accuracy_test)\n",
    "          \n",
    "        # Get average result from all k-fold splits\n",
    "        feature_accuracy = np.mean(test_accuracy_results)\n",
    "    \n",
    "        # Update chosen feature and result if this feature is a new best\n",
    "        if feature_accuracy > best_result:\n",
    "            best_result = feature_accuracy\n",
    "            best_feature = feature\n",
    "    \n",
    "    # k-fold splits are complete    \n",
    "    # Add mean accuracy and AUC to record of accuracy by feature number\n",
    "    accuracy_by_feature_number.append(best_result)\n",
    "    chosen_features.append(best_feature)\n",
    "    available_features.remove(best_feature)\n",
    "\n",
    "# Put results in DataFrame\n",
    "results = pd.DataFrame()\n",
    "results['feature to add'] = chosen_features\n",
    "results['accuracy'] = accuracy_by_feature_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "chart_x = list(range(1, maximum_features_to_choose+1))\n",
    "\n",
    "plt.plot(chart_x, accuracy_by_feature_number,\n",
    "        label = 'Accuracy')\n",
    "\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! By selecting our best features from our expanded model we now have a little over 84% accuracy! It looks like we need about 15 features for our optimum model, nearly all of which are polynomial terms. Note that sklearn's polynomial method outputs features names in relation to the original X index. Our 'best' feature is a product of X1 and X10. Let's see what those are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_index_names = list(X_df)\n",
    "print ('X1:',X_index_names[1])\n",
    "print ('X10:',X_index_names[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at just the ages of male passengers is the best signle predictor of survival."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
