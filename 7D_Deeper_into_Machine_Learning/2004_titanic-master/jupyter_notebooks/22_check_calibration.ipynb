{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - checking model calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as classifying a case into a specific class (e.g. survived or died), it may be useful to look at the probability that the model assigns to a case being a specific class. Examples of when probability may be important include:\n",
    "\n",
    "* High risk models (e.g. medical) â€“ to be clear on risk being taken\n",
    "* When probability thresholds are important to decision-making (e.g. screening when a relatively low probability may still signify further action should be taken).\n",
    "* Improving our models - focussing on mistakes with v.high or v.low probabilities.\n",
    "\n",
    "When we are interested in probabilities, it is important to be able to trust the probabilities reported by the model. This involves two steps:\n",
    "\n",
    "1. Check whether model probabilities are well calibrated\n",
    "2. If necessary re-calibrate the model\n",
    "\n",
    "In this work book we will look at checking model calibration. We will first perform a single run working through the steps 'manually', and then put it in to a stratified k-fold loop using sklearn's `calibration_curve` method.\n",
    "\n",
    "When checking probability calibration it is important, just like when check accuracy, that we test calibration on a test set that has not been used to train the model. Like checking accuracy, we can also use stratified k-fold validation to get a more accurate assessment of calibration (which we will use when we use sklearn's built-in methods).\n",
    "\n",
    "If you are unfamiliar with stratfied k-fold validation for replication of machine learning testing, please see:\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/03_k_fold.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import machine learning methods\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The section below downloads pre-processed data, and saves it to a subfolder (from where this code is run).\n",
    "If data has already been downloaded that cell may be skipped.\n",
    "\n",
    "Code that was used to pre-process the data ready for machine learning may be found at:\n",
    "\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = False\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, load data and remove passenger index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into X (features) and y (labels)\n",
    "\n",
    "We will separate out our features (the data we use to make a prediction) from our label (what we are truing to predict).\n",
    "By convention our features are called `X` (usually upper case to denote multiple features), and the label (survive or not) `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run method\n",
    "\n",
    "Here we will go through the steps of calibration to illustrate how it is performed. We will then put this code inside a stratified k-fold loop below.\n",
    "\n",
    "### Divide into training and calibration test sets\n",
    "\n",
    "We will divide the data in training and calibration test sets (75:25 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and fit model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict probabilities for calibration\n",
    "\n",
    "Now we can use the trained model to predict the probability of survival using the model's `predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_calibrate_probabilities = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliablity plot\n",
    "\n",
    "In the reliability plot we will bin cases by their predicted probability, into 10 bins of probability of surviving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin data with numpy digitize (this will assign a bin to each case)\n",
    "step = 0.10\n",
    "bins = np.arange(step, 1+step, step)\n",
    "digitized = np.digitize(y_calibrate_probabilities, bins)\n",
    "\n",
    "# Put data in DataFrame\n",
    "reliability = pd.DataFrame()\n",
    "reliability['bin'] = digitized\n",
    "reliability['probability'] = y_calibrate_probabilities\n",
    "reliability['observed'] = y_test.values\n",
    "\n",
    "# Summarise data by bin in new dataframe\n",
    "reliability_summary = pd.DataFrame()\n",
    "\n",
    "# Add bins to summary\n",
    "reliability_summary['bin'] = bins\n",
    "\n",
    "# Calculate mean of predicted probability of survival for each bin\n",
    "reliability_summary['confidence'] = \\\n",
    "    reliability.groupby('bin').mean()['probability']\n",
    "\n",
    "# Calculate the proportion of passengers who survive in each bin\n",
    "reliability_summary['fraction_positive'] = \\\n",
    "    reliability.groupby('bin').mean()['observed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results:\n",
    "\n",
    "In a perfectly calibrated model, the fraction of passengers who survive should be the same as the average probability of survival in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(reliability_summary['confidence'], \n",
    "         reliability_summary['fraction_positive'],\n",
    "         linestyle='-',\n",
    "         marker='o',\n",
    "         label='model')\n",
    "\n",
    "plt.plot([0,1],[0,1],\n",
    "         linestyle='--',\n",
    "         label='theoretical')\n",
    "\n",
    "plt.xlabel('Model probability')\n",
    "plt.ylabel('Fraction positive')\n",
    "\n",
    "plt.title('Reliability plot')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to look for in a Reliability Plot:\n",
    "\n",
    "* Run the plot multiple times - is there a consistent pattern? See below for code using sklearn methods that allows for easy replication of reliability plots.\n",
    "* Points below the diagonal: The model has over-forecast - the calculated probabilities are too large.\n",
    "* Points above the diagonal: The model has under-forecast - the calculated probabilities are too small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stratfied k-fold validation and sklearn's calibration curve method\n",
    "\n",
    "The method below will take the output probabilities from the Random Forest model and use sklearn's `calibration_curve` method to create bins of model probability and the fraction positive in each bin. We will use 'quantile' binning which creates bins of equal size. Use of uniform bins (bins of uniform width) can cause problems if a bin is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Convert data to NumPy arrays (required for stratified k-fold)\n",
    "X_np = X.values\n",
    "y_np = y.values\n",
    "\n",
    "# Set up k-fold splits\n",
    "number_of_splits = 5\n",
    "skf = StratifiedKFold(n_splits = number_of_splits, shuffle=True, \n",
    "                      random_state=42)\n",
    "skf.get_n_splits(X_np, y_np)\n",
    "\n",
    "# Define bins\n",
    "number_of_bins = 10\n",
    "\n",
    "# Set up results DataFrames (to get results from each run)\n",
    "results_model_probability = pd.DataFrame()\n",
    "results_fraction_positive = pd.DataFrame()\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "loop_counter = 0\n",
    "for train_index, test_index in skf.split(X_np, y_np):    \n",
    "        \n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "    \n",
    "    # Set up and fit model\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Get test set proabilities\n",
    "    y_calibrate_probabilities = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Get calibration curve (use quantile to make sure all bins exist)\n",
    "    fraction_pos, model_prob = calibration_curve(\n",
    "        y_test, y_calibrate_probabilities, \n",
    "        n_bins=number_of_bins,\n",
    "        strategy='quantile')    \n",
    "\n",
    "    # record run results\n",
    "    results_model_probability[loop_counter] = model_prob\n",
    "    results_fraction_positive[loop_counter] = fraction_pos\n",
    "    \n",
    "    # Increment loop counter\n",
    "    loop_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "Plot individual runs and means for bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Add individual k-fold runs\n",
    "for run in range(number_of_splits):\n",
    "    plt.plot(results_model_probability[run],\n",
    "             results_fraction_positive[run],\n",
    "             linestyle='--',\n",
    "             linewidth=0.75,\n",
    "             color='0.5')\n",
    "    \n",
    "# Add mean\n",
    "plt.plot(results_model_probability.mean(axis=1),\n",
    "         results_fraction_positive.mean(axis=1),\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color='darkorange',\n",
    "         label='mean')\n",
    "\n",
    "# Add diagonal\n",
    "plt.plot([0,1],[0,1],\n",
    "         linestyle='--',\n",
    "         label='theoretical')\n",
    "\n",
    "plt.xlabel('Model probability')\n",
    "plt.ylabel('Fraction positive')\n",
    "\n",
    "plt.title('Reliability plot')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Using k-fold splits and taking the mean we can see our model probability is reasonably well calibrated.\n",
    "* Testing the model calibration gives us confidence that the probabilities reported by the model are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do do if a model is poorly calibrated\n",
    "\n",
    "If a model is poorly calibrated (if the calibration curve is significantly offset from the diagonal) then the model may be recalibrated. The most common method of recalibration is to take the probability output of the calibration data set(s) and fit a logistic regression model using those outputs and the known `y` values (survived or not) for those probabilities. This is known as *Platt scaling*. Another method is known as isotonic scaling, but is prone to over-fitting, so is only suitable for large data sets.\n",
    "\n",
    "Platt scaling may be easily implemented by adding in a logistic regression model fitted to the output of the calibration test. Platt scaling and isotonic scaling may also be performed by using the `CalibratedClassifier` method built in to sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
